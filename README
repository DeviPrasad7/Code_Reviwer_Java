# ğŸš€ Code Correction Model with T5

---

## ğŸ“ Project Overview

This project implements a **code correction model** using the powerful [T5 (Text-to-Text Transfer Transformer)](https://huggingface.co/docs/transformers/model_doc/t5) architecture.  
The model learns to fix buggy code snippets by training on paired datasets of buggy and corrected code.  
It leverages [Hugging Face Transformers](https://huggingface.co/docs/transformers/index), [Datasets](https://huggingface.co/docs/datasets/index), and is optimized for use with **Google Colab** and **Google Drive**.

---

## âœ¨ Features

- ğŸ“‚ Reads buggy and fixed code data from text files stored in Google Drive
- ğŸ¤– Utilizes T5 tokenizer and model for sequence-to-sequence learning
- ğŸ§¹ Includes data cleaning and tokenization pipelines for efficient preprocessing
- ğŸ’¾ Supports checkpoint saving and resuming during training
- ğŸ—ƒï¸ Saves the trained model directly to Google Drive
- ğŸ‘€ Provides lightweight evaluation via sample output inspection

---

## âš™ï¸ Requirements

- Python 3.x
- [`transformers`](https://huggingface.co/docs/transformers/index)
- [`datasets`](https://huggingface.co/docs/datasets/index)
- `torch`
- Google Colab *(optional, but recommended)*
- Access to Google Drive for dataset storage and model checkpointing

---

## ğŸš¦ How to Run

1. **Mount your Google Drive** in Colab:
    ```python
    from google.colab import drive
    drive.mount('/content/drive')
    ```
2. **Prepare your dataset folders** on Drive, each containing:
    - `buggy.txt`
    - `fixed.txt`
3. **Set the notebook dataset folder paths** accordingly.
4. **Run the notebook cells sequentially** to:
    - Load, clean, preprocess, and tokenize the data
    - Train the model
5. **Model checkpoints and the final model** will be saved to your Google Drive.

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!  
Feel free to open an issue or submit a pull request.

---

## ğŸ“§ Contact

For questions or support, please open an issue in this
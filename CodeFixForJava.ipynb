{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
    },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports and setup\n",
        "import os\n",
        "from datasets import DatasetDict, Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import torch\n",
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "qjRis0qAiu3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eNUmu_xixYV",
        "outputId": "4d577d3a-8159-443c-a491-6a80ab2e7ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Function to load data from folder\n",
        "def load_data_from_folder(folder_path):\n",
        "    buggy_path = os.path.join(folder_path, \"buggy.txt\")\n",
        "    fixed_path = os.path.join(folder_path, \"fixed.txt\")\n",
        "\n",
        "    with open(buggy_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        buggy_lines = f.readlines()\n",
        "\n",
        "    with open(fixed_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        fixed_lines = f.readlines()\n",
        "\n",
        "    assert len(buggy_lines) == len(fixed_lines), \"Mismatch in buggy and fixed lines\"\n",
        "\n",
        "    data = {\"input_text\": buggy_lines, \"target_text\": fixed_lines}\n",
        "    return Dataset.from_dict(data)"
      ],
      "metadata": {
        "id": "2dU40Pdai5zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Load datasets\n",
        "train_folder = \"/content/drive/My Drive/CodeFix_DataSet/train\"\n",
        "eval_folder = \"/content/drive/My Drive/CodeFix_DataSet/eval\"\n",
        "test_folder = \"/content/drive/My Drive/CodeFix_DataSet/test\"\n",
        "\n",
        "train_dataset = load_data_from_folder(train_folder)\n",
        "eval_dataset = load_data_from_folder(eval_folder)\n",
        "test_dataset = load_data_from_folder(test_folder)\n",
        "\n",
        "datasets = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": eval_dataset,\n",
        "    \"test\": test_dataset,\n",
        "})"
      ],
      "metadata": {
        "id": "f5-D8iSki71u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Clean data - strip whitespace from strings\n",
        "def strip_strings(example):\n",
        "    example[\"input_text\"] = example[\"input_text\"].strip()\n",
        "    example[\"target_text\"] = example[\"target_text\"].strip()\n",
        "    return example\n",
        "\n",
        "datasets[\"train\"] = datasets[\"train\"].map(strip_strings)\n",
        "datasets[\"validation\"] = datasets[\"validation\"].map(strip_strings)\n",
        "datasets[\"test\"] = datasets[\"test\"].map(strip_strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "bd0df4c044334c4fb7eb4a14a5830b50",
            "8aafe26223c34fdfa111fd14bdcb8ae0",
            "1a3a729671804cae96511f6b647ab7ec",
            "bd90f0d09c7748da97359bce17386514",
            "42a5ebd2decb48aa85332e576c59284d",
            "1e7aac4de7c1425fb5cf37321a1505bb",
            "fe71db1f2f244af182be63d9442e097e",
            "b0480c95d01043a8ae5cd3b5f35caffc",
            "ca3e3fb9064d468ca7ed41433bb18c02",
            "159c25cdb5994ef08ea4d19b23b600db",
            "931c6b63cf6c48c6a3e937a3dd908b05",
            "3dc8760961ef43f8ab98b2cfecf6abf4",
            "54feb42ec4dc4847907b91a128aad48a",
            "38e68e2ef38340909e0937b7864901b7",
            "8837a79d8b484a7a8c85bbe9682e1ce8",
            "1a02abc3bc1a45c494381c6520e6215e",
            "8c36f349674b4cb79c12b8653ed5f7f0",
            "ef4ca5d0cc9542e0b4b625a3cb83a6f9",
            "629b175181f24ec1a65e72ae61594d3c",
            "5acf4826c26a4132b63c0599b662b54f",
            "2327689c714a40aeb79e9a3d817160e9",
            "b4054374b26149d7af83082c7f1e02bb",
            "da0209a403854e03985d3cb9df870eb4",
            "be6cc23eadf848538605bb773bff10b4",
            "5b948245ce2d46ada8a97e13edb4a16b",
            "904642c6e99c40a98ee3f0b4fd51ad47",
            "c1185a9afbc64c0eab243d50ebdf6f86",
            "86b84ead27524189a94ce1c3d9592275",
            "464de5f0d3ff4dbead67b231167b74b5",
            "bf136eeffb534a3ea6291cbc6446673d",
            "8fb70310137c4c078abe2eed285a3137",
            "f8b255c4e33843518bf523b00d9b8eec",
            "6052e78f149d44e1a50dd973fe1e787c"
          ]
        },
        "id": "LIBvMt6Qi9vJ",
        "outputId": "c66e0244-3c98-4300-c38a-cb3abf8c5163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/46680 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd0df4c044334c4fb7eb4a14a5830b50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5835 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dc8760961ef43f8ab98b2cfecf6abf4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5835 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da0209a403854e03985d3cb9df870eb4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Tokenizer and model init\n",
        "model_name = \"t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "oImbaSMvjA_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Preprocessing before tokenizing\n",
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"fix code: {code.strip()}\" for code in examples[\"input_text\"]]\n",
        "    targets = [code.strip() for code in examples[\"target_text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "9AvpSb-qjDAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Tokenize datasets\n",
        "tokenized_datasets = datasets.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "44a2b7cc21714a3e987a4d9859c29463",
            "7efaaa75ebaf466990c0bb9f0ec9c3d1",
            "2c7578d2ac224b0b85491000609cadba",
            "9f6f04d41f554c669a2e09293bc843a1",
            "67e2191e3c8449c1bb67e54a80cc8311",
            "9090c969ef2d4b408eb655bba2530add",
            "c9c8e9e30be8414fa6e1dfcd02c6145d",
            "e2cd1e83d3954bb892b410cc28ac34dc",
            "5046f2b2f11d4acab411f22b895008e5",
            "a18f7edf3e8b487bbac8a0c9d1322c80",
            "d0290f1f220b4924aa856053dffe4d5b",
            "a3289553c44147559331b0fbf69bbe12",
            "5f77cae23c61433c8e471d0a1391dd3b",
            "d5b0e63e8d214eeeb8a66b42fcb6d82a",
            "e39040d5f792474382aec32697d14424",
            "f64b4ecefd4b4c7fb3b06a64f1648027",
            "b366756b5efc4e798ad4d164b0fc8004",
            "fdb5d15722a847a69f053d29baed7d29",
            "ff52d600954443a490bf54f7cf9c1b1f",
            "b2efee1891df4e789b585c8f2b9db2a5",
            "8fbf3bf4f6b340ebbd20fd351c48a084",
            "844fdc42fe36481c86db0d3e93c9572d",
            "6a5b5f7d0b68469597ef792e7b134c61",
            "1dae50e7347f4b8295324f0f28349a10",
            "aeb39b2ab1df4ac9a511724c898833b7",
            "28faf07ec44c4ba29c19411b1f468a93",
            "027dc0fd9b27419e974828f4ba0ebc4e",
            "4796e0a476014c428e2b5968fa94d9b5",
            "4d9518878a7e4a92ae11860a1eaab68c",
            "cde7c204e22048b0848891a2a9d0a00b",
            "0cb1966de96d4176b2706429e8ebbb79",
            "3aa47c07786d4d918ca85e6fbbefd0cb",
            "83ea4b780e4a439293dc3b7f4df692d8"
          ]
        },
        "id": "k0lxZFJxjE93",
        "outputId": "48337064-541a-4ec4-cc7e-d4c51ad6fbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/46680 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44a2b7cc21714a3e987a4d9859c29463"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4006: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5835 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3289553c44147559331b0fbf69bbe12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5835 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a5b5f7d0b68469597ef792e7b134c61"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Checkpoint directory on Drive\n",
        "checkpoint_dir = \"/content/drive/MyDrive/code_fix_model_checkpoint\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "GQXSm-ThjG1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Setup training args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=checkpoint_dir,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=3,\n",
        "    max_grad_norm=1.0,\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "leO0-P10jJDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPpvwKEujN8G",
        "outputId": "08a444aa-af4a-47c4-f846-d47598ce5b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1617776094.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Find latest checkpoint to resume if any\n",
        "def find_latest_checkpoint(path):\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    checkpoints = [os.path.join(path, d) for d in os.listdir(path) if d.startswith(\"checkpoint\")]\n",
        "    if not checkpoints:\n",
        "        return None\n",
        "    latest_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
        "    return latest_checkpoint"
      ],
      "metadata": {
        "id": "4HqNS8tmjTsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Train or resume\n",
        "last_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
        "if last_checkpoint:\n",
        "    print(f\"Resuming from checkpoint: {last_checkpoint}\")\n",
        "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else:\n",
        "    trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "rvVFQBNmjWBY",
        "outputId": "b07d1393-e71c-4ab2-988e-ca68c13fe986"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='35010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  125/35010 33:15 < 157:10:29, 0.06 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Save model and tokenizer to Drive\n",
        "trainer.save_model(checkpoint_dir)\n",
        "tokenizer.save_pretrained(checkpoint_dir)"
      ],
      "metadata": {
        "id": "JtVdfzjpjYhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: Extra: Quick dataset samples check (you can run anytime)\n",
        "print(f\"Train size: {len(datasets['train'])}, Eval size: {len(datasets['validation'])}, Test size: {len(datasets['test'])}\")\n",
        "\n",
        "import random\n",
        "print(\"Random buggy code samples:\")\n",
        "for _ in range(3):\n",
        "    i = random.randint(0, len(datasets['train']) - 1)\n",
        "    print(datasets[\"train\"][i][\"input_text\"])\n"
      ],
      "metadata": {
        "id": "zygiL4BVjaU3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
Project Overview:
  This project implements a code correction model using the T5 (Text-to-Text Transfer Transformer) architecture. The model learns to fix buggy code snippets from paired datasets containing buggy and fixed code     samples. Training is done using Hugging Face's transformers and datasets libraries, and leverages Google Colab with Google Drive for storage.

Features:
  Reads buggy and fixed code data from text files stored in Google Drive.
  Uses T5 tokenizer and model for sequence-to-sequence training.
  Includes data cleaning and tokenization pipelines for efficient preprocessing.
  Training supports checkpoint saving and resuming.
  Outputs a trained model saved on Google Drive.
  Lightweight evaluation via sample output inspection.

Requirements:
  Python 3.x
  transformers library
  datasets library
  torch
  Google Colab (optional, but used in this notebook)
  Access to Google Drive for dataset storage and model checkpointing

How to Run:
  Mount your Google Drive in Colab
  Prepare your dataset folders on Drive, each containing buggy.txt and fixed.txt files.
  Set the notebook dataset folder paths accordingly.
  Run cells sequentially to load, clean, preprocess, tokenize, and train the model.
  Model checkpoints and the final model get saved to Google Drive.
